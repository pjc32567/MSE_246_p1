log.model <- glm(vote_2008~age+ideology+race+sex,data = df.train,family = "binomial")
coef(log.model)['sexmale']
coef(log.model)['sexfemale']
coef(log.model)['age30-44']
coef(log.model)['age45-64']
coef(log.model)['age65+']
coef(log.model)['age18-29']
## 2
df.test$prob <- predict(log.model,newdata = df.test, type = "response")
df.test$classes <- rep('barack obama',nrow(df.test))
df.test$classes[df.test$prob>0.5] <- 'john mcCain'
df.table <- table(df.test$classes,df.test$vote_2008)
library(caret)
confusionMatrix(df.table)
## 2 c
library(ROCR)
pred <- prediction(df.test$prob, df.test$vote_2008=='barack obama')
perf <- performance(pred, "tpr", "fpr")
plot(perf)
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
## 2 d
df.test.true <- df.test[df.test$vote_2008=='barack obama',]
df.test.false <- df.test[df.test$vote_2008!='barack obama',]
count <- 0
for(i in 1:10000){
a = df.test.true$prob[sample(nrow(df.test.true),1)]
b = df.test.false$prob[sample(nrow(df.test.false),1)]
if(a>=b){
count = count+1
}
}
AUC_compare <- count/10000
## 2 e
df.test$probObama <- 1-df.test$prob
df.test$interval <- cut_width(df.test$probObama,0.1,boundary = 0)
df.test$obama <- ifelse(df.test$vote_2008=='barack obama',1,0)
library(dplyr)
group_by_interval <- group_by(df.test,interval)
by_interval <- summarise(group_by_interval, avg_pred = mean(probObama),
total_samples = n(),
frac_obama = sum(obama)/total_samples)
View(by_interval)
by_interval$interval
type(by_interval$interval)
class(by_interval$interval)
runif(1000)
cut_width(runif(1000),10)
cut_width(runif(1000),0.1)
cut_width(runif(1000),0.1,boundary = 0)
by_interval$interval <- unique(cut_width(runif(1000),0.1,boundary = 0))
intervals <- unique(cut_width(runif(1000),0.1,boundary = 0))
class(intervals)
full_join(intervals,by_interval)
intervals <- as.data.frame(unique(cut_width(runif(1000),0.1,boundary = 0)))
full_join(intervals,by_interval)
?full_join
left_join(intervals,by_interval,by = c("intervals"="interval"))
View(intervals)
colnames(intervals) <- c("interval")
left_join(intervals,by_interval,by = c("interval"="interval"))
final <- left_join(intervals,by_interval,by = c("interval"="interval"))
View(final)
final <- full_join(intervals,by_interval,by = c("interval"="interval"))
final[is.na(final)] <- 0
library(ggplot2)
library(ggplot2)
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))
library(ggplot2)
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
library(Hmisc)
library(gdata)
final$interval <- reorder.factor(final$interval,unique(cut_width(runif(1000),0.1,boundary = 0)))
final$interval <- reorder.factor(final$interval,levels = unique(cut_width(runif(1000),0.1,boundary = 0)))
final$interval <- reorder.factor(final$interval,levels = unique(cut_width(runif(1000),0.1,boundary = 0)))
df.test$probObama <- 1-df.test$prob
df.test$interval <- cut_width(df.test$probObama,0.1,boundary = 0)
df.test$obama <- ifelse(df.test$vote_2008=='barack obama',1,0)
library(dplyr)
group_by_interval <- group_by(df.test,interval)
by_interval <- summarise(group_by_interval, avg_pred = mean(probObama),
total_samples = n(),
frac_obama = sum(obama)/total_samples)
intervals <- as.data.frame(unique(cut_width(runif(1000),0.1,boundary = 0)))
colnames(intervals) <- c("interval")
final <- left_join(intervals,by_interval,by = c("interval"="interval"))
final[is.na(final)] <- 0
library(gdata)
final$interval <- reorder.factor(final$interval,levels = unique(cut_width(runif(1000),0.1,boundary = 0)))
library(ggplot2)
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
unique(cut_width(runif(1000),0.1,boundary = 0))
levels(unique(cut_width(runif(1000),0.1,boundary = 0)))
final$interval <- reorder.factor(final$interval,levels =levels(unique(cut_width(runif(1000),0.1,boundary = 0))))
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
df2 <- read.table("https://5harad.com/mse125/assets/hw6/poll_data_full.tsv", sep = "\t", header = TRUE)
train2.rows <- sample(nrow(df2),floor(0.7*nrow(df2)))
df2.train <- df[train2.rows,]
df2.test <- df[-train2.rows,]
log2.model <- glm(vote_2008~age+ideology+race+sex,data = df2.train,family = "binomial")
df2.test$prob <- predict(log2.model,newdata = df2.test, type = "response")
View(df2.test)
View(df2.test)
df2$majorparty <- ifelse(df2$vote_2008=='barack obama'&df2$vote_2008=='john mcCain',1,0)
train2.rows <- sample(nrow(df2),floor(0.7*nrow(df2)))
df2.train <- df[train2.rows,]
df2.test <- df[-train2.rows,]
log2.model <- glm(majorparty~age+ideology+race+sex,data = df2.train,family = "binomial")
df2.test$prob <- predict(log2.model,newdata = df2.test, type = "response")
View(df2)
df2$majorparty <- ifelse(df2$vote_2008=='barack obama' | df2$vote_2008=='john mcCain',1,0)
train2.rows <- sample(nrow(df2),floor(0.7*nrow(df2)))
df2.train <- df[train2.rows,]
df2.test <- df[-train2.rows,]
log2.model <- glm(majorparty~age+ideology+race+sex,data = df2.train,family = "binomial")
train2.rows <- sample(nrow(df2),floor(0.7*nrow(df2)))
df2.train <- df2[train2.rows,]
df2.test <- df2[-train2.rows,]
log2.model <- glm(majorparty~age+ideology+race+sex,data = df2.train,family = "binomial")
df2.test$prob <- predict(log2.model,newdata = df2.test, type = "response")
View(df2.test)
summary(log2.model)
hist(df2.test$prob)
df_major <- filter(df2,majorparty==1)
df_major <- filter(df2,majorparty==1)
train3.rows <- sample(nrow(df_major),floor(0.7*nrow(df_major)))
df3.train <- df_major[train3.rows,]
df3.test <- df_major[-train3.rows,]
log3.model <- glm(vote_2008~age+ideology+race+sex,data = df3.train,family = "binomial")
df3.test$prob <- predict(log3.model,newdata = df3.test, type = "response")
hist(df3.test$prob)
unique(df3.test$vote_2008)
sum(df_major$majorparty)==nrow(df_major)
View(df3.test)
test <- filter(df3.test,prob>0.4,prob<0.6)
View(test)
hist(test$sex)
barchart(test$sex)
View(df3.test)
row.names(df3.test)
df2$prob1 <- predict(log2.model,newdata = df2,type = "response")
df2$prob2 <- predict(log3.model,newdata = df2,type = "response")
predict(log3.model,newdata = df2[1,],type = "response")
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2$prob2 <- predict(log2.model,newdata = df2[i,],type = "response")
}
}
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
set.seed(123)
## 1
df<- read.table("https://5harad.com/mse125/assets/hw6/poll_data.tsv", sep = "\t", header = TRUE)
head(df)
summary(df)
train.rows <- sample(nrow(df),floor(0.7*nrow(df)))
df.train <- df[train.rows,]
df.test <- df[-train.rows,]
log.model <- glm(vote_2008~age+ideology+race+sex,data = df.train,family = "binomial")
coef(log.model)['sexmale']
coef(log.model)['sexfemale']
coef(log.model)['age30-44']
coef(log.model)['age45-64']
coef(log.model)['age65+']
coef(log.model)['age18-29']
## 2
df.test$prob <- predict(log.model,newdata = df.test, type = "response")
df.test$classes <- rep('barack obama',nrow(df.test))
df.test$classes[df.test$prob>0.5] <- 'john mcCain'
df.table <- table(df.test$classes,df.test$vote_2008)
library(caret)
confusionMatrix(df.table)
## 2 c
library(ROCR)
pred <- prediction(df.test$prob, df.test$vote_2008=='barack obama')
perf <- performance(pred, "tpr", "fpr")
plot(perf)
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
## 2 d
df.test.true <- df.test[df.test$vote_2008=='barack obama',]
df.test.false <- df.test[df.test$vote_2008!='barack obama',]
count <- 0
for(i in 1:10000){
a = df.test.true$prob[sample(nrow(df.test.true),1)]
b = df.test.false$prob[sample(nrow(df.test.false),1)]
if(a>=b){
count = count+1
}
}
AUC_compare <- count/10000
## 2 e
df.test$probObama <- 1-df.test$prob
df.test$interval <- cut_width(df.test$probObama,0.1,boundary = 0)
df.test$obama <- ifelse(df.test$vote_2008=='barack obama',1,0)
library(dplyr)
group_by_interval <- group_by(df.test,interval)
by_interval <- summarise(group_by_interval, avg_pred = mean(probObama),
total_samples = n(),
frac_obama = sum(obama)/total_samples)
intervals <- as.data.frame(unique(cut_width(runif(1000),0.1,boundary = 0)))
colnames(intervals) <- c("interval")
final <- left_join(intervals,by_interval,by = c("interval"="interval"))
final[is.na(final)] <- 0
library(gdata)
#final$interval <- reorder.factor(final$interval,levels =levels(unique(cut_width(runif(1000),0.1,boundary = 0))))
library(ggplot2)
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
## 3
df2 <- read.table("https://5harad.com/mse125/assets/hw6/poll_data_full.tsv", sep = "\t", header = TRUE)
df2$majorparty <- ifelse(df2$vote_2008=='barack obama' | df2$vote_2008=='john mcCain',1,0)
train2.rows <- sample(nrow(df2),floor(0.7*nrow(df2)))
df2.train <- df2[train2.rows,]
df2.test <- df2[-train2.rows,]
log2.model <- glm(majorparty~age+ideology+race+sex,data = df2.train,family = "binomial")
df2.test$prob <- predict(log2.model,newdata = df2.test, type = "response")
##
df_major <- filter(df2,majorparty==1)
train3.rows <- sample(nrow(df_major),floor(0.7*nrow(df_major)))
df3.train <- df_major[train3.rows,]
df3.test <- df_major[-train3.rows,]
log3.model <- glm(vote_2008~age+ideology+race+sex,data = df3.train,family = "binomial")
df3.test$prob <- predict(log3.model,newdata = df3.test, type = "response")
hist(df3.test$prob)
test <- filter(df3.test,prob>0.4,prob<0.6)
unique(df3.test$vote_2008)
df2$prob1 <- predict(log2.model,newdata = df2,type = "response")
df2$prob2 <- 1
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
log3.model <- glm(vote_2008~age+race+sex,data = df3.train,family = "binomial")
df3.test$prob <- predict(log3.model,newdata = df3.test, type = "response")
hist(df3.test$prob)
test <- filter(df3.test,prob>0.4,prob<0.6)
unique(df3.test$vote_2008)
df2$prob1 <- predict(log2.model,newdata = df2,type = "response")
df2$prob2 <- 1
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
df2[1,]$'vote_2008'
df2[1,]$vote_2008
df2[100,]$vote_2008
df2[100,]$vote_2008!='other'
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
hist(df2$prob2)
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2[i,]$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
View(df2)
hist(df2$prob2)
log3.model <- glm(vote_2008~age+race+sex,data = df3.train,family = "binomial")
hist(df2$prob2)
hist(df3.test$prob)
df_major <- filter(df2,majorparty==1)
train3.rows <- sample(nrow(df_major),floor(0.7*nrow(df_major)))
df3.train <- df_major[train3.rows,]
df3.test <- df_major[-train3.rows,]
log3.model <- glm(vote_2008~party+age+race+sex,data = df3.train,family = "binomial")
df3.test$prob <- predict(log3.model,newdata = df3.test, type = "response")
hist(df3.test$prob)
test <- filter(df3.test,prob>0.4,prob<0.6)
unique(df3.test$vote_2008)
df2$prob1 <- predict(log2.model,newdata = df2,type = "response")
df2$prob2 <- 1
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2[i,]$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
hist(df2$prob2)
hist(df3.test$prob)
hist(df2$prob2)
hist(df2$prob1)
table(df2$prob1)
df2 <- read.table("https://5harad.com/mse125/assets/hw6/poll_data_full.tsv", sep = "\t", header = TRUE)
df2$majorparty <- ifelse(df2$vote_2008=='barack obama' | df2$vote_2008=='john mcCain',1,0)
df2$majorparty <- as.factor(df2$majorparty)
train2.rows <- sample(nrow(df2),floor(0.7*nrow(df2)))
df2.train <- df2[train2.rows,]
df2.test <- df2[-train2.rows,]
log2.model <- glm(majorparty~age+ideology+race+sex,data = df2.train,family = "binomial")
df2.test$prob <- predict(log2.model,newdata = df2.test, type = "response")
##
df_major <- filter(df2,majorparty==1)
train3.rows <- sample(nrow(df_major),floor(0.7*nrow(df_major)))
df3.train <- df_major[train3.rows,]
df3.test <- df_major[-train3.rows,]
log3.model <- glm(vote_2008~party+age+race+sex,data = df3.train,family = "binomial")
df3.test$prob <- predict(log3.model,newdata = df3.test, type = "response")
hist(df3.test$prob)
test <- filter(df3.test,prob>0.4,prob<0.6)
unique(df3.test$vote_2008)
df2$prob1 <- predict(log2.model,newdata = df2,type = "response")
df2$prob2 <- 1
for(i in 1:nrow(df2))
{
if(df2[i,]$vote_2008!='other')
{
df2[i,]$prob2 <- predict(log3.model,newdata = df2[i,],type = "response")
}
}
hist(df2$prob1)
table(df2$majorparty)
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
final$interval <- reorder.factor(final$interval,levels =levels(unique(cut_width(runif(1000),0.1,boundary = 0))))
View(final)
final$interval <- reorder.factor(final$interval,new.order = levels(unique(cut_width(runif(1000),0.1,boundary = 0))) )
View(final)
levels(unique(cut_width(runif(1000),0.1,boundary = 0)))
final <- left_join(intervals,by_interval,by = c("interval"="interval"))
final[is.na(final)] <- 0
final$interval <- reorder.factor(final$interval,new.order = levels(unique(cut_width(runif(1000),0.1,boundary = 0))) )
View(final)
ggplot(data = final,mapping = aes(x = interval))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
View(final)
ggplot(data = final,mapping = aes(x = interval,size = total_samples))+geom_point(mapping = aes(y = avg_pred))+
geom_point(mapping = aes(y = frac_obama))
ggplot(data = final,mapping = aes(x = interval,size = total_samples))+geom_point(mapping = aes(y = avg_pred,color = blue,size = total_samples))+
geom_point(mapping = aes(y = frac_obama))
ggplot(data = final,mapping = aes(x = interval,size = total_samples))+geom_point(mapping = aes(y = avg_pred,color = 'blue',size = total_samples))+
geom_point(mapping = aes(y = frac_obama))
ggplot(data = final,mapping = aes(x = interval,size = total_samples))+geom_point(mapping = aes(y = avg_pred,size = total_samples))+
geom_point(mapping = aes(y = frac_obama))
setwd("~/GitHub/MSE_246_p1")
df <- mutate(df,dayselapsed = as.numeric(difftime(strptime(ChargeOffDate, format = "%m/%d/%Y"),
strptime(ApprovalDate, format = "%m/%d/%Y"))))
# This file will do an initial data cleaning/addition of the SBA loan dataset
# Authors: Daniel, Brent, Peng and Yash
# File Description:
# Inputs:
# Outputs:
library(dplyr)
library(GGally)
library(survival)
library(ROCR)
set.seed(1)
df <- read.csv("SBA_loan_data_new.csv")
# Macroeconomic data sets
unemployment_data <- read.csv("emp-unemployment.csv")
# FED Interest Rate data per year
# Source: https://fred.stlouisfed.org/series/DFF/downloaddata
interest_rates_data <- read.csv("FED_IR.csv")
# Total Economic Cost due to Tornados per year per state
# Source: http://www.spc.noaa.gov/wcm/test.html (manipulated using R and excel)
Tornado_damage_economic <- read.csv("Tornado FL.csv")
# Small Business Lending data stats per state for 2011
# Source:
# 1. https://www.sba.gov/advocacy/firm-size-data
# 2. https://www.sba.gov/content/small-business-lending-united-states-2010-2011
Small_business_lending_data <- read.csv("Small_Business_lending_stats.csv")
SBLR <- c(rep(0,length(df$BorrState)))
for (i in 1:length(df$BorrState)) {
SBLR[i]=Small_business_lending_data[match(df$BorrState[i],Small_business_lending_data$State),5]
}
#Add small business loan ratio (total small business loans issued/total small businesses per state )
df=cbind(df,SBLR)
# Average HPI Index by State for 1990-2016
hpi_state <- read.csv("hpi_state.csv", header=TRUE, stringsAsFactors=F)
hpi_state$ProjectState = as.factor(hpi_state$ProjectState)
df <- left_join(df, hpi_state, by=c("ProjectState", "ApprovalFiscalYear"))
df$mn_hpi = as.numeric(df$mn_hpi)
# Filtering out Loans which are canceled or exempt
df <- filter(df, LoanStatus != "CANCLD")
df <- filter(df, LoanStatus != "EXEMPT")
df <- filter(df, DeliveryMethod != "504REFI")
# Removing the first 3 columns of the data set  as they contain the common program info and a
# unique identifying name for every business
df <- df[,-c(1,2,3)]
# Removing the column BorrZip Code
df <- df[,-c(3)]
# Removing CDC street, city, zip
df <- df[,-c(4,5,6,7)]
# Adding 4 columns - isDefault, NotSameState, ThirdPartyApproved and dayselapsed
df$isDefault <- ifelse(df$LoanStatus=="CHGOFF",1,0)
df$isDefault <- factor(df$isDefault)
df$isDefault <- as.numeric(df$isDefault)
df$NotSameState <- factor(df$NotSameState)
df$ThirdPartyApproved <- factor(df$ThirdPartyApproved)
df <- mutate(df,dayselapsed = as.numeric(difftime(strptime(ChargeOffDate, format = "%m/%d/%Y"),
strptime(ApprovalDate, format = "%m/%d/%Y"))))
df$dayselapsed[is.na(df$dayselapsed)] <- 7300
# Removing the Third Party Lender Name, City, State
df <- df[,-c(4,5,6)]
# Removing subpgmdesc
df <- df[,-c(9)]
# Removing NAICS description
df <- df[,-c(12)]
# Removing Project County and State
df <- df[,-c(12,13)]
df <- mutate(df,Naics2digits = substr(NaicsCode,1,2))
# Writing the cleaned dataframe to a csv file
write.csv(df,file = "SBA_cleaned_data.csv")
?tmerge
library(survival)
?tmerge
interest_rates=matrix(0,nrow=length(df$ApprovalFiscalYear),ncol=length(1990:2014))
View(interest_rates)
# This file will do an initial data cleaning/addition of the SBA loan dataset
# Authors: Daniel, Brent, Peng and Yash
# File Description:
# Inputs:
# Outputs:
library(dplyr)
library(GGally)
library(survival)
library(ROCR)
set.seed(1)
df <- read.csv("SBA_loan_data_new.csv")
# Macroeconomic data sets
unemployment_data <- read.csv("emp-unemployment.csv")
# FED Interest Rate data per year
# Source: https://fred.stlouisfed.org/series/DFF/downloaddata
interest_rates_data <- read.csv("FED_IR.csv")
# Total Economic Cost due to Tornados per year per state
# Source: http://www.spc.noaa.gov/wcm/test.html (manipulated using R and excel)
Tornado_damage_economic <- read.csv("Tornado FL.csv")
# Small Business Lending data stats per state for 2011
# Source:
# 1. https://www.sba.gov/advocacy/firm-size-data
# 2. https://www.sba.gov/content/small-business-lending-united-states-2010-2011
Small_business_lending_data <- read.csv("Small_Business_lending_stats.csv")
SBLR <- c(rep(0,length(df$BorrState)))
for (i in 1:length(df$BorrState)) {
SBLR[i]=Small_business_lending_data[match(df$BorrState[i],Small_business_lending_data$State),5]
}
#Add small business loan ratio (total small business loans issued/total small businesses per state )
df=cbind(df,SBLR)
# Average HPI Index by State for 1990-2016
hpi_state <- read.csv("hpi_state.csv", header=TRUE, stringsAsFactors=F)
hpi_state$ProjectState = as.factor(hpi_state$ProjectState)
df <- left_join(df, hpi_state, by=c("ProjectState", "ApprovalFiscalYear"))
df$mn_hpi = as.numeric(df$mn_hpi)
# Filtering out Loans which are canceled or exempt
df <- filter(df, LoanStatus != "CANCLD")
df <- filter(df, LoanStatus != "EXEMPT")
df <- filter(df, DeliveryMethod != "504REFI")
# Removing the first 3 columns of the data set  as they contain the common program info and a
# unique identifying name for every business
df <- df[,-c(1,2,3)]
# Removing the column BorrZip Code
df <- df[,-c(3)]
# Removing CDC street, city, zip
df <- df[,-c(4,5,6,7)]
# Adding 4 columns - isDefault, NotSameState, ThirdPartyApproved and dayselapsed
df$isDefault <- ifelse(df$LoanStatus=="CHGOFF",1,0)
df$isDefault <- factor(df$isDefault)
df$isDefault <- as.numeric(df$isDefault)
df$NotSameState <- factor(df$NotSameState)
df$ThirdPartyApproved <- factor(df$ThirdPartyApproved)
df <- mutate(df,dayselapsed = as.numeric(difftime(strptime(ChargeOffDate, format = "%m/%d/%Y"),
strptime(ApprovalDate, format = "%m/%d/%Y"))))
df$dayselapsed[is.na(df$dayselapsed)] <- 7300
# Removing the Third Party Lender Name, City, State
df <- df[,-c(4,5,6)]
# Removing subpgmdesc
df <- df[,-c(9)]
# Removing NAICS description
df <- df[,-c(12)]
# Removing Project County and State
df <- df[,-c(12,13)]
df <- mutate(df,Naics2digits = substr(NaicsCode,1,2))
interest_rates=matrix(0,nrow=length(df$ApprovalFiscalYear),ncol=length(1990:2014))
for (i in 1:length(1990:2014)) {
interest_rates[,i]=interest_rates_data[match(1989+i,interest_rates_data$Year),2]
}
for (i in 1:length(df$ApprovalFiscalYear)){
if ((round(df$dayselapsed[i]/365)+1990) < 2014) {
j = 2014-(round(df$dayselapsed[i]/365)+1990)
column_final=length(1990:(round(df$dayselapsed[i]/365)+1990))
for (n in  column_final:25){
interest_rates[i,n]==0}
}
}
# Writing the cleaned dataframe to a csv file
write.csv(df,file = "SBA_cleaned_data.csv")
View(df)
View(interest_rates)
